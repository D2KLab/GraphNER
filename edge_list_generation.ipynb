{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/opt/tmp/huggingface/datasets/conll2003/conll2003/1.0.0/26b70ce2b0f32cb35a27151dbfa2dbe88c82bcdaf8f29433bcdc612a9b314e83)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = pd.read_csv('conceptnet_en.csv')\n",
    "cn_isa = pd.read_csv('data/conceptnet_isa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['film', 'band', 'weapon', 'album', 'device', 'book', 'company',\n",
       "       'musical_artist', 'tangible_thing', 'person', 'river',\n",
       "       'given_name', 'software', 'city', 'disease', 'capital',\n",
       "       'agent_non_geographical', 'area', 'event', 'activity', 'state',\n",
       "       'human', 'magazine', 'asian', 'person_with_nationality', 'country',\n",
       "       'administrative_region', 'region', 'place', 'ethnic_group',\n",
       "       'soccer_player', 'african', 'organization', 'people', 'structure',\n",
       "       'station', 'artifact', 'part', 'location', 'town'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_isa.object.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_keys = set(cn.subject.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1165189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cn_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bona_fide', 'bona_fide', 'bonanza', 'bondage', 'bone', 'bone',\n",
       "       'bone', 'bone', 'bone', 'boo', 'book', 'book', 'book',\n",
       "       'book_smart', 'boom', 'boon', 'bore', 'bore', 'bore', 'boreal',\n",
       "       'boreal', 'boreas', 'boreas'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.subject.values[1800:1823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2labels = {}\n",
    "for w in cn_isa.subject.unique():\n",
    "    subcn = cn_isa[cn_isa.subject == w]\n",
    "    word2labels[w] = subcn.object.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word2labels, open('edges/word2labels.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conceptnet_isa.csv\t       eng.testa\t  eng.train\r\n",
      "conll2003_sparse_dev.pickle    eng.testa.openNLP  eng.train.openNLP\r\n",
      "conll2003_sparse_test.pickle   eng.testb\t  ws2_40r\r\n",
      "conll2003_sparse_train.pickle  eng.testb.openNLP  ws3_40r\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
       "  ['Peter', 'Blackburn'],\n",
       "  ['BRUSSELS', '1996-08-22']],\n",
       " [['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.'],\n",
       "  ['NNP', 'NNP'],\n",
       "  ['NNP', 'CD']],\n",
       " [['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
       "  ['B-PER', 'I-PER'],\n",
       "  ['B-LOC', 'O']])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['words'][:3], dataset['train']['pos'][:3], dataset['train']['ner'][:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"!$%&'*+,-.:;<=>?@`\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['!', '$', '%', '&', \"'\", '*', '+', ',', '-', '.', ':', ';', '<', '=', '>', '?', '@', '`'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec80e531ac9479b9af2f60a0a975bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='TRAIN', max=14041.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd71779f7804bc88efd7c3bebfcb50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='VALIDATION', max=3250.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6a8cbb12f74fff99fee4a7956cfa9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='TEST', max=3453.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "punctuation = ['!', '$', '%', '&', \"'\", '*', '+', ',', '-', '.', ':', ';', '<', '=', '>', '?', '@', '`']\n",
    "\n",
    "vocabulary = {}\n",
    "data = {}\n",
    "\n",
    "special_cases = []\n",
    "special_num_cases = []\n",
    "special_O_cases = []\n",
    "\n",
    "for split in['train', 'validation', 'test']:\n",
    "    # print(split)\n",
    "    data[split] = []\n",
    "    vocabulary[split] = set()\n",
    "    \n",
    "    for doc in tqdm(dataset[split], desc=split.upper()):\n",
    "        tokens, labels, extras = [], [], []\n",
    "        \n",
    "        for token, pos, label in zip(doc['words'], doc['pos'], doc['ner']):\n",
    "            if token == pos:\n",
    "                continue # this is punctuation\n",
    "            \n",
    "            elif pos == ',':\n",
    "                pos = 'NNP'\n",
    "            \n",
    "            if token.endswith('='):\n",
    "                token = token[:-1]\n",
    "            \n",
    "            while token and token[0] in punctuation:\n",
    "                token = token[1:]\n",
    "            \n",
    "            token = re.sub(r'\\d+', '<NUM>', token)\n",
    "            token = token.replace('`', \"'\")\n",
    "            \n",
    "            if not token:\n",
    "                continue\n",
    "            \n",
    "            if all([c in ',.-' for c in token.split('<NUM>')]):\n",
    "                special_num_cases.append((token, label))\n",
    "                token = '<NUM>'\n",
    "            elif not token.isalpha() and label != 'O':\n",
    "                special_cases.append((token, label))\n",
    "            elif not token.isalpha() and label == 'O':\n",
    "                special_O_cases.append((token, label))\n",
    "            \n",
    "            extra = ['<'+pos.lower()+'>']\n",
    "            if token.lower() in word2labels:\n",
    "                extra.extend(['<'+l.lower()+'>' for l in word2labels[token.lower()]])\n",
    "                \n",
    "            if token.lower() not in cn_keys:\n",
    "                extra.append('<not_in_dict>')\n",
    "            if token == token.upper():\n",
    "                extra.append('<all_caps>')\n",
    "            if token.count('.') > 0 and (token.count('.') + 1) == len(token.split('.')): # C.J or C.J.\n",
    "                extra.append('<accronym>')\n",
    "            if token[0] == token[0].upper() and token[:1] == token[:1].lower(): \n",
    "                extra.append('<capitalized>')\n",
    "                \n",
    "                \n",
    "            vocabulary[split].add(token.lower())\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "            extras.append(extra)\n",
    "            \n",
    "\n",
    "        data[split].append((tokens, labels, extras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb'],\n",
       " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O'],\n",
       " [['<nnp>', '<all_caps>'],\n",
       "  ['<vbz>'],\n",
       "  ['<jj>', '<human>', '<person>', '<person_with_nationality>'],\n",
       "  ['<nn>'],\n",
       "  ['<to>'],\n",
       "  ['<vb>'],\n",
       "  ['<jj>', '<person>', '<person_with_nationality>'],\n",
       "  ['<nn>']])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<weapon>', '<capitalized>', '<wp$>', '<in>', '<cd>', '<jjs>', '<vbd>', '<wrb>', '<jj>', '<rbr>', '<band>', '<human>', '<ex>', '<(>', '<to>', '<state>', '<disease>', '<fw>', '<vbg>', '<film>', '<agent_non_geographical>', '<dt>', '<artifact>', '<asian>', '<given_name>', '<nns>', '<wp>', '<soccer_player>', '<part>', '<vbz>', '<location>', '<city>', '<vbp>', '<vbn>', '<jjr>', '<person_with_nationality>', '<book>', '<river>', '<pdt>', '<rb>', '<software>', '<all_caps>', '<organization>', '<town>', '<african>', '<region>', '<wdt>', '<nnps>', '<pos>', '<not_in_dict>', '<vb>', '<people>', '<musical_artist>', '<ls>', '<nnp>', '<company>', '<station>', '<country>', '<area>', '<sym>', '<nn|sym>', '<activity>', '<$>', '<ethnic_group>', '<album>', '<uh>', '<prp>', '<rp>', '<administrative_region>', '<magazine>', '<event>', '<prp$>', '<person>', '<tangible_thing>', '<place>', '<nn>', '<cc>', '<rbs>', '<device>', '<capital>', '<md>', '<)>', '<accronym>', '<structure>']\n"
     ]
    }
   ],
   "source": [
    "extra_vocab = list(set([e for example in data['train'] for l in example[2] for e in l]))\n",
    "print(extra_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(set([x[0] for x in special_O_cases]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21647\n"
     ]
    }
   ],
   "source": [
    "all_voc = set([w for split in vocabulary for w in vocabulary[split]])\n",
    "print(len(all_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17292, 7754, 7161]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(vocabulary[split]) for split in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [x for l in data['train'] for x in l[0]]\n",
    "train_counter = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 32515,\n",
       "         'B-LOC': 1668,\n",
       "         'B-PER': 1617,\n",
       "         'I-PER': 1156,\n",
       "         'I-LOC': 257,\n",
       "         'B-MISC': 702,\n",
       "         'I-MISC': 210,\n",
       "         'B-ORG': 1661,\n",
       "         'I-ORG': 830})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_counter = Counter([x for l in data['test'] for x in l[1]])\n",
    "train_labels_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602\n"
     ]
    }
   ],
   "source": [
    "intials = []\n",
    "accronyms = []\n",
    "whatelse = []\n",
    "hyphenated = []\n",
    "\n",
    "for term, label in special_cases:\n",
    "    if term == term.upper() and term.count('.') > 0 and term.count('.') == len(term.split('.')) - 1 and len(term) <= 2:\n",
    "        intials.append((term, label))\n",
    "    elif term == term.upper() and term.count('.') > 0 and term.count('.') == len(term.split('.')) - 1 and len(term) > 2:\n",
    "        accronyms.append((term, label))\n",
    "    elif '-' in term and len(term.split('-')) > 1 and  (term.split('-')[0] == term.split('-')[0].lower() or (term.split('-')[1] == term.split('-')[1].lower())):\n",
    "        hyphenated.append((term, label))\n",
    "    else:\n",
    "        whatelse.append((term, label))\n",
    "# print('\\n'.join(str(c) for c in set(whatelse)))\n",
    "print(len(whatelse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17377"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vocab = sorted(list(vocabulary['train']) + ['<span>'] + list(extra_vocab))\n",
    "len(final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '<$>',\n",
       " '<(>',\n",
       " '<)>',\n",
       " '<accronym>',\n",
       " '<activity>',\n",
       " '<administrative_region>',\n",
       " '<african>',\n",
       " '<agent_non_geographical>',\n",
       " '<album>',\n",
       " '<all_caps>',\n",
       " '<area>',\n",
       " '<artifact>',\n",
       " '<asian>',\n",
       " '<band>',\n",
       " '<book>',\n",
       " '<capital>',\n",
       " '<capitalized>',\n",
       " '<cc>',\n",
       " '<cd>',\n",
       " '<city>',\n",
       " '<company>',\n",
       " '<country>',\n",
       " '<device>',\n",
       " '<disease>',\n",
       " '<dt>',\n",
       " '<ethnic_group>',\n",
       " '<event>',\n",
       " '<ex>',\n",
       " '<film>',\n",
       " '<fw>',\n",
       " '<given_name>',\n",
       " '<human>',\n",
       " '<in>',\n",
       " '<jj>',\n",
       " '<jjr>',\n",
       " '<jjs>',\n",
       " '<location>',\n",
       " '<ls>',\n",
       " '<magazine>',\n",
       " '<md>',\n",
       " '<musical_artist>',\n",
       " '<nn>',\n",
       " '<nnp>',\n",
       " '<nnps>',\n",
       " '<nns>',\n",
       " '<nn|sym>',\n",
       " '<not_in_dict>',\n",
       " '<num>',\n",
       " '<num>)<num>',\n",
       " '<num>*',\n",
       " '<num>,<num>-a',\n",
       " '<num>,<num>-acre',\n",
       " '<num>,<num>-b',\n",
       " '<num>,<num>-hectare',\n",
       " '<num>,<num>-km',\n",
       " '<num>,<num>-seat',\n",
       " '<num>,<num>-strong',\n",
       " '<num>,<num>-student',\n",
       " '<num>,<num>nd',\n",
       " '<num>,<num>th',\n",
       " '<num>--',\n",
       " '<num>--<num>-<num>-<num>-<num>',\n",
       " '<num>--ruehe',\n",
       " '<num>-<num>(<num>-<num>',\n",
       " '<num>-<num>-<num>--',\n",
       " '<num>-<num>/<num>',\n",
       " '<num>-<num>c',\n",
       " '<num>-<num>f',\n",
       " '<num>-a-share',\n",
       " '<num>-all',\n",
       " '<num>-an-hour',\n",
       " '<num>-ball',\n",
       " '<num>-cent',\n",
       " '<num>-club',\n",
       " '<num>-day',\n",
       " '<num>-foot',\n",
       " '<num>-for-<num>',\n",
       " '<num>-game',\n",
       " '<num>-hour',\n",
       " '<num>-inch',\n",
       " '<num>-inning',\n",
       " '<num>-iron',\n",
       " '<num>-km',\n",
       " '<num>-kms',\n",
       " '<num>-man',\n",
       " '<num>-member',\n",
       " '<num>-metre',\n",
       " '<num>-mile',\n",
       " '<num>-million']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vocab[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {w:i for i,w in enumerate(final_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7858"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['ismail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17377"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_vocab, open('edges/vocabulary.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74555c12837a492b98a54e8d5f7b028e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='TRAIN', max=14041.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffd6ac130fd40aba22c232dc37ea6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='VALIDATION', max=3250.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f5ad41196c4a468bfb2f093f8c3cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='TEST', max=3453.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "before_edges = {w: [] for w in final_vocab}\n",
    "after_edges  = {w: [] for w in final_vocab}\n",
    "isa_edges    = {w: [] for w in final_vocab}\n",
    "vocab_dict   = {w: [] for w in final_vocab}\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "for split in data:\n",
    "    for example in tqdm(data[split], desc=split.upper()):\n",
    "        text = [w.lower() for w in example[0]]\n",
    "        for i, word in enumerate(text):\n",
    "            term = word.lower()\n",
    "            if term not in vocab_dict: # new words appearing only in the eval and test\n",
    "                term = '<span>'\n",
    "            left_context  = text[max(i-window_size, 0):i] + ([] if i >= window_size else ['<span>'])\n",
    "            right_context = text[i+1:i+1+window_size] + ([] if i + window_size < len(text) else ['<span>'])\n",
    "            left_context  = [w if w in vocab_dict else '<span>' for w in left_context]\n",
    "            right_context = [w if w in vocab_dict else '<span>' for w in right_context]\n",
    "            isa_context   = example[2][i]\n",
    "            \n",
    "            before_edges[term].extend(right_context)\n",
    "            after_edges[term].extend(left_context)\n",
    "            isa_edges[term].extend(isa_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<span>',\n",
       " '<num>',\n",
       " 'city',\n",
       " 'off',\n",
       " 'record',\n",
       " 'top',\n",
       " 'back',\n",
       " 'end',\n",
       " 'japanese',\n",
       " 'market',\n",
       " 'open',\n",
       " 'police',\n",
       " 'put',\n",
       " 'quarter',\n",
       " 'south',\n",
       " 'area',\n",
       " 'balance',\n",
       " 'base',\n",
       " 'book',\n",
       " 'bucharest']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l[0] for l in sorted(isa_edges.items(), key=lambda k: len(set(k[1])), reverse=True)[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list_before = []\n",
    "edge_list_after  = []\n",
    "edge_list_isa    = []\n",
    "\n",
    "for word in vocab_dict:\n",
    "    edge_list_before.extend((word2id[word], word2id[w]) for w in before_edges[word])\n",
    "    edge_list_after.extend((word2id[word], word2id[w]) for w in after_edges[word])\n",
    "    edge_list_isa.extend((word2id[word], word2id[w]) for w in isa_edges[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505816, 505816, 460646)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_list_before), len(edge_list_after), len(edge_list_isa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220835, 221104, 32046)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(edge_list_before)), len(set(edge_list_after)), len(set(edge_list_isa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473985, 401452, 40487)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_all_edges = len(set(edge_list_before)) + len(set(edge_list_after)) + len(set(edge_list_isa))\n",
    "edges_list_unique = sorted(set(edge_list_before).union(set(edge_list_after)))\n",
    "len_unique_context_edges = len(edges_list_unique)\n",
    "len_all_edges, len_unique_context_edges, len_all_edges - len_unique_context_edges - len(set(edge_list_isa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list_all = sorted(set(edge_list_before).union(set(edge_list_after).union(set(edge_list_isa))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433498"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20M_classifier_all_available_features.ipynb   nodes_classifier.ipynb\r\n",
      "conceptnet_en.csv\t\t\t      onehot_pytorch_lightning.ipynb\r\n",
      "data\t\t\t\t\t      prepare_dataset.ipynb\r\n",
      "edge_list_generation.ipynb\t\t      README.md\r\n",
      "edges\t\t\t\t\t      runs\r\n",
      "graph_embeddings_generation.ipynb\t      snap\r\n",
      "GraphNER_binary_representation_pytorch.ipynb  tempGraph.emb\r\n",
      "hparams.yaml\t\t\t\t      tempGraph.graph\r\n",
      "lightning_logs\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((48, 48), 21248),\n",
       " ((48, 186), 11338),\n",
       " ((186, 186), 2662),\n",
       " ((7470, 15542), 1553),\n",
       " ((10757, 15542), 1445),\n",
       " ((15542, 186), 1335),\n",
       " ((15542, 10757), 1170),\n",
       " ((186, 48), 1152),\n",
       " ((13451, 186), 948),\n",
       " ((15736, 15542), 836)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(edge_list_before).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vocab[7470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '<$>',\n",
       " '<(>',\n",
       " '<)>',\n",
       " '<accronym>',\n",
       " '<activity>',\n",
       " '<administrative_region>',\n",
       " '<african>',\n",
       " '<agent_non_geographical>',\n",
       " '<album>',\n",
       " '<all_caps>',\n",
       " '<area>',\n",
       " '<artifact>',\n",
       " '<asian>',\n",
       " '<band>',\n",
       " '<book>',\n",
       " '<capital>',\n",
       " '<capitalized>',\n",
       " '<cc>',\n",
       " '<cd>',\n",
       " '<city>',\n",
       " '<company>',\n",
       " '<country>',\n",
       " '<device>',\n",
       " '<disease>',\n",
       " '<dt>',\n",
       " '<ethnic_group>',\n",
       " '<event>',\n",
       " '<ex>',\n",
       " '<film>',\n",
       " '<fw>',\n",
       " '<given_name>',\n",
       " '<human>',\n",
       " '<in>',\n",
       " '<jj>',\n",
       " '<jjr>',\n",
       " '<jjs>',\n",
       " '<location>',\n",
       " '<ls>',\n",
       " '<magazine>',\n",
       " '<md>',\n",
       " '<musical_artist>',\n",
       " '<nn>',\n",
       " '<nnp>',\n",
       " '<nnps>',\n",
       " '<nns>',\n",
       " '<nn|sym>',\n",
       " '<not_in_dict>',\n",
       " '<num>',\n",
       " '<num>)<num>',\n",
       " '<num>*',\n",
       " '<num>,<num>-a',\n",
       " '<num>,<num>-acre',\n",
       " '<num>,<num>-b',\n",
       " '<num>,<num>-hectare',\n",
       " '<num>,<num>-km',\n",
       " '<num>,<num>-seat',\n",
       " '<num>,<num>-strong',\n",
       " '<num>,<num>-student',\n",
       " '<num>,<num>nd',\n",
       " '<num>,<num>th',\n",
       " '<num>--',\n",
       " '<num>--<num>-<num>-<num>-<num>',\n",
       " '<num>--ruehe',\n",
       " '<num>-<num>(<num>-<num>',\n",
       " '<num>-<num>-<num>--',\n",
       " '<num>-<num>/<num>',\n",
       " '<num>-<num>c',\n",
       " '<num>-<num>f',\n",
       " '<num>-a-share',\n",
       " '<num>-all',\n",
       " '<num>-an-hour',\n",
       " '<num>-ball',\n",
       " '<num>-cent',\n",
       " '<num>-club',\n",
       " '<num>-day',\n",
       " '<num>-foot',\n",
       " '<num>-for-<num>',\n",
       " '<num>-game',\n",
       " '<num>-hour',\n",
       " '<num>-inch',\n",
       " '<num>-inning',\n",
       " '<num>-iron',\n",
       " '<num>-km',\n",
       " '<num>-kms',\n",
       " '<num>-man',\n",
       " '<num>-member',\n",
       " '<num>-metre',\n",
       " '<num>-mile',\n",
       " '<num>-million',\n",
       " '<num>-minute',\n",
       " '<num>-month',\n",
       " '<num>-month-old',\n",
       " '<num>-mth',\n",
       " '<num>-ounce',\n",
       " '<num>-page',\n",
       " '<num>-party',\n",
       " '<num>-pct',\n",
       " '<num>-pound',\n",
       " '<num>-seat']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_lists = {'before_edges': edge_list_before,\n",
    "              'after_edges': edge_list_after,\n",
    "              'isa_edges': edge_list_isa,\n",
    "              'context_edges': edges_list_unique,\n",
    "              'all_edges': edges_list_all,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in edge_lists:\n",
    "    pickle.dump(edge_lists[filename], open('edges/' + filename + '.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in edge_lists:\n",
    "    with open('edges/' + filename + '.edgelist', 'w') as f:\n",
    "        for s, t in edge_lists[filename]:\n",
    "            f.write(f'{s} {t}\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
