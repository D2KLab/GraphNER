{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fad385852f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92af8f4667524945b67fa2adb392704e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178610.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7664267f5af64876b6a262fb16497a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=44900.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dd68e53ac545de84832b72d92e657b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40760.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = {'train': [], 'validation': [], 'test': []}\n",
    "dataset_path = '/data/graphner_embeddings/ae_emb_npy_2000_15epochs/'\n",
    "\n",
    "for split in dataset:\n",
    "    files_list = os.listdir(dataset_path+split)\n",
    "    for i, filename in tqdm(enumerate(sorted(files_list)), total=len(files_list)):\n",
    "        dataset[split].append(pickle.load(open(dataset_path+split+'/'+str(i)+'.pickle', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': 0, 'MISC': 1, 'O': 2, 'ORG': 3, 'PER': 4}\n"
     ]
    }
   ],
   "source": [
    "labels = pickle.load(open('labels.pickle', 'rb'))\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, split, label2id=label2id):\n",
    "        X, Y = zip(*dataset[split])\n",
    "\n",
    "        self.X = [torch.tensor(x) for x in X]\n",
    "        self.Y = [torch.tensor(y) for y in Y]\n",
    "        self.X_len = len(X)\n",
    "        self.labels = sorted(label2id.keys())\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "        x.requires_grad = False\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def Y(self):\n",
    "        return self.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(dataset, 'train')\n",
    "dev_set = Dataset(dataset, 'validation')\n",
    "test_set = Dataset(dataset, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2000])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 0\n",
    "for local_features, local_labels in train_loader:\n",
    "    input_dim = local_features.shape[1]\n",
    "    print(local_features.shape)\n",
    "    print(local_labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 144631, 4: 11124, 3: 9984, 0: 8288, 1: 4583})\n"
     ]
    }
   ],
   "source": [
    "training_counter = Counter([y.item() for y in train_set.Y])\n",
    "print(training_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOC', 'MISC', 'O', 'ORG', 'PER']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(batch_X, batch_Y, model, optimizer, loss_fn):\n",
    "    Y_hat = model(batch_X)\n",
    "    loss = loss_fn(Y_hat, batch_Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=input_dim, hidden_dim=512, output_dim=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fch = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # extra layers layers\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnet = FeedForwardNetwork(dropout_rate=0.2, hidden_dim=1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {'loss/train': {}, 'dev': {}}\n",
    "writer = SummaryWriter(comment='xp5-autoreg-wei2-lr1e3-mom0.9-wd5e4-hd1024-dr0.2-bs64-dim2000-15', log_dir=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7436, 1.0000, 0.1780, 0.6775, 0.6419], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "label_counter   = Counter([y.item() for y in train_set.Y])\n",
    "labels_freqs    = [label_counter[label] / sum(label_counter.values()) for label in range(len(labels))]\n",
    "labels_weights1 = [min(label_counter.values()) / label_counter[label] for label in range(len(labels))]\n",
    "labels_weights2 = [np.sqrt(min(label_counter.values())) / np.sqrt(label_counter[label]) for label in range(len(labels))]\n",
    "\n",
    "weights = torch.Tensor(labels_weights2).to(device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_params = {'lr': 5e-3, \n",
    "                    'momentum': 0.9, \n",
    "                    'weight_decay': 5e-4,\n",
    "                   }\n",
    "\n",
    "log_interval = int(len(train_loader) / 2)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.SGD(ffnet.parameters(), **optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166bca270355405c890d1a68ea871daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2791.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [0/178610 (0%)]\tLoss: 0.473480\n",
      "Train Epoch: 32 [89280/178610 (50%)]\tLoss: 0.308125\n",
      "Train Epoch: 32 [139500/178610 (100%)]\tLoss: 0.448107\n",
      "\n",
      "Average loss on epoch 32: 0.446914252793381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1824287f591d44d5b59951c58a3297cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=702.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7564    0.7367    0.7465      2150\n",
      "           1     0.6377    0.4745    0.5441      1703\n",
      "           2     0.9605    0.9920    0.9760     35185\n",
      "           3     0.5315    0.5034    0.5171      2179\n",
      "           4     0.8412    0.7165    0.7739      3683\n",
      "\n",
      "    accuracy                         0.9138     44900\n",
      "   macro avg     0.7455    0.6846    0.7115     44900\n",
      "weighted avg     0.9079    0.9138    0.9098     44900\n",
      "\n",
      "CPU times: user 15.5 s, sys: 4.32 s, total: 19.9 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs = 1\n",
    "\n",
    "for epoch in range(len(logs['loss/train']), len(logs['loss/train']) + max_epochs):\n",
    "    \n",
    "    # Training\n",
    "    ffnet.train()\n",
    "    print('Epoch', epoch)\n",
    "    logs['loss/train'][epoch] = []\n",
    "    writer.add_scalar(\"Learning_rate\", optimizer_params['lr'], epoch)\n",
    "\n",
    "    for batch, (batch_X, batch_Y) in enumerate(tqdm(train_loader)):\n",
    "        # tranfer to GPU\n",
    "        batch_X, batch_Y = batch_X.float().to(device), batch_Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        l = backprop(batch_X, batch_Y, ffnet, optimizer, loss_fn)\n",
    "        logs['loss/train'][epoch].append(l)\n",
    "        \n",
    "        if batch % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(batch_X), len(train_loader.dataset),\n",
    "                100. * batch / len(train_loader), l))\n",
    "    \n",
    "    logs['loss/train'][epoch] = np.mean(logs['loss/train'][epoch])\n",
    "    writer.add_scalar(\"Loss/train\", logs['loss/train'][epoch], epoch)\n",
    "    print(f'Average loss on epoch {epoch}: {logs[\"loss/train\"][epoch]}')\n",
    "    \n",
    "    # Validation\n",
    "    ffnet.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        gt = []\n",
    "        for batch, (batch_X, batch_Y) in enumerate(tqdm(dev_loader)):\n",
    "            # Transfer to GPU\n",
    "            batch_X = batch_X.float().to(device)\n",
    "            output = nn.Softmax(dim=1)(ffnet(batch_X))\n",
    "            preds.append(output.cpu())\n",
    "            gt.append(batch_Y)\n",
    "\n",
    "        all_out = [np.argmax(l) for batch in preds for l in batch.numpy()]\n",
    "        all_gt  = [l for batch in gt for l in batch.numpy()]\n",
    "\n",
    "        print(classification_report(all_out, all_gt, digits=4))\n",
    "\n",
    "        micro_F1 = metrics.f1_score(all_gt, all_out, average='micro')\n",
    "        macro_F1 = metrics.f1_score(all_gt, all_out, average='macro')\n",
    "        weighted_F1 = metrics.f1_score(all_gt, all_out, average='weighted')\n",
    "        writer.add_scalar(\"micro_F1/dev\", micro_F1, epoch)\n",
    "        writer.add_scalar(\"macro_F1/dev\", macro_F1, epoch)\n",
    "        writer.add_scalar(\"weighted_F1/dev\", weighted_F1, epoch)\n",
    "        logs['dev'][epoch] = (micro_F1, weighted_F1, macro_F1, (all_gt, all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6ff12442bd41ebbb7ef1eee841b015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=637.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7231    0.7109    0.7170      1958\n",
      "           1     0.6558    0.3825    0.4831      1574\n",
      "           2     0.9500    0.9928    0.9709     31242\n",
      "           3     0.5483    0.5280    0.5380      2589\n",
      "           4     0.8330    0.6800    0.7488      3397\n",
      "\n",
      "    accuracy                         0.9001     40760\n",
      "   macro avg     0.7420    0.6588    0.6916     40760\n",
      "weighted avg     0.8925    0.9001    0.8939     40760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ffnet.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    gt = []\n",
    "    for batch, (batch_X, batch_Y) in enumerate(tqdm(test_loader)):\n",
    "        # Transfer to GPU\n",
    "        batch_X = batch_X.float().to(device)\n",
    "        output = nn.Softmax(dim=1)(ffnet(batch_X))\n",
    "        preds.append(output.cpu())\n",
    "        gt.append(batch_Y)\n",
    "\n",
    "    all_out = [np.argmax(l) for batch in preds for l in batch.numpy()]\n",
    "    all_gt  = [l for batch in gt for l in batch.numpy()]\n",
    "\n",
    "    print(classification_report(all_out, all_gt, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
