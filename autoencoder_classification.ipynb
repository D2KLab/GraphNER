{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'train': [], 'validation': [], 'test': []}\n",
    "dataset_path = '/data/graphner_embeddings/ae_emb_npy_2000_15epochs/'\n",
    "\n",
    "for split in dataset:\n",
    "    files_list = os.listdir(dataset_path+split)\n",
    "    for i, filename in tqdm(enumerate(sorted(files_list)), total=len(files_list)):\n",
    "        dataset[split].append(pickle.load(open(dataset_path+split+'/'+str(i)+'.pickle', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pickle.load(open('labels.pickle', 'rb'))\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, split, label2id=label2id):\n",
    "        X, Y = zip(*dataset[split])\n",
    "\n",
    "        self.X = [torch.tensor(x) for x in X]\n",
    "        self.Y = [torch.tensor(y) for y in Y]\n",
    "        self.X_len = len(X)\n",
    "        self.labels = sorted(label2id.keys())\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "        x.requires_grad = False\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def Y(self):\n",
    "        return self.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(dataset, 'train')\n",
    "dev_set = Dataset(dataset, 'validation')\n",
    "test_set = Dataset(dataset, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 0\n",
    "for local_features, local_labels in train_loader:\n",
    "    input_dim = local_features.shape[1]\n",
    "    print(local_features.shape)\n",
    "    print(local_labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_counter = Counter([y.item() for y in train_set.Y])\n",
    "print(training_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(batch_X, batch_Y, model, optimizer, loss_fn):\n",
    "    Y_hat = model(batch_X)\n",
    "    loss = loss_fn(Y_hat, batch_Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=input_dim, hidden_dim=512, output_dim=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fch = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # extra layers layers\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnet = FeedForwardNetwork(dropout_rate=0.2, hidden_dim=1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {'loss/train': {}, 'dev': {}}\n",
    "writer = SummaryWriter(comment='xp5-autoreg-wei2-lr1e3-mom0.9-wd5e4-hd1024-dr0.2-bs64-dim2000-15', log_dir=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counter   = Counter([y.item() for y in train_set.Y])\n",
    "labels_freqs    = [label_counter[label] / sum(label_counter.values()) for label in range(len(labels))]\n",
    "labels_weights1 = [min(label_counter.values()) / label_counter[label] for label in range(len(labels))]\n",
    "labels_weights2 = [np.sqrt(min(label_counter.values())) / np.sqrt(label_counter[label]) for label in range(len(labels))]\n",
    "\n",
    "weights = torch.Tensor(labels_weights2).to(device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_params = {'lr': 5e-3, \n",
    "                    'momentum': 0.9, \n",
    "                    'weight_decay': 5e-4,\n",
    "                   }\n",
    "\n",
    "log_interval = int(len(train_loader) / 2)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.SGD(ffnet.parameters(), **optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_epochs = 1\n",
    "\n",
    "for epoch in range(len(logs['loss/train']), len(logs['loss/train']) + max_epochs):\n",
    "    \n",
    "    # Training\n",
    "    ffnet.train()\n",
    "    print('Epoch', epoch)\n",
    "    logs['loss/train'][epoch] = []\n",
    "    writer.add_scalar(\"Learning_rate\", optimizer_params['lr'], epoch)\n",
    "\n",
    "    for batch, (batch_X, batch_Y) in enumerate(tqdm(train_loader)):\n",
    "        # tranfer to GPU\n",
    "        batch_X, batch_Y = batch_X.float().to(device), batch_Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        l = backprop(batch_X, batch_Y, ffnet, optimizer, loss_fn)\n",
    "        logs['loss/train'][epoch].append(l)\n",
    "        \n",
    "        if batch % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(batch_X), len(train_loader.dataset),\n",
    "                100. * batch / len(train_loader), l))\n",
    "    \n",
    "    logs['loss/train'][epoch] = np.mean(logs['loss/train'][epoch])\n",
    "    writer.add_scalar(\"Loss/train\", logs['loss/train'][epoch], epoch)\n",
    "    print(f'Average loss on epoch {epoch}: {logs[\"loss/train\"][epoch]}')\n",
    "    \n",
    "    # Validation\n",
    "    ffnet.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        gt = []\n",
    "        for batch, (batch_X, batch_Y) in enumerate(tqdm(dev_loader)):\n",
    "            # Transfer to GPU\n",
    "            batch_X = batch_X.float().to(device)\n",
    "            output = nn.Softmax(dim=1)(ffnet(batch_X))\n",
    "            preds.append(output.cpu())\n",
    "            gt.append(batch_Y)\n",
    "\n",
    "        all_out = [np.argmax(l) for batch in preds for l in batch.numpy()]\n",
    "        all_gt  = [l for batch in gt for l in batch.numpy()]\n",
    "\n",
    "        print(classification_report(all_out, all_gt, digits=4))\n",
    "\n",
    "        micro_F1 = metrics.f1_score(all_gt, all_out, average='micro')\n",
    "        macro_F1 = metrics.f1_score(all_gt, all_out, average='macro')\n",
    "        weighted_F1 = metrics.f1_score(all_gt, all_out, average='weighted')\n",
    "        writer.add_scalar(\"micro_F1/dev\", micro_F1, epoch)\n",
    "        writer.add_scalar(\"macro_F1/dev\", macro_F1, epoch)\n",
    "        writer.add_scalar(\"weighted_F1/dev\", weighted_F1, epoch)\n",
    "        logs['dev'][epoch] = (micro_F1, weighted_F1, macro_F1, (all_gt, all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnet.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    gt = []\n",
    "    for batch, (batch_X, batch_Y) in enumerate(tqdm(test_loader)):\n",
    "        # Transfer to GPU\n",
    "        batch_X = batch_X.float().to(device)\n",
    "        output = nn.Softmax(dim=1)(ffnet(batch_X))\n",
    "        preds.append(output.cpu())\n",
    "        gt.append(batch_Y)\n",
    "\n",
    "    all_out = [np.argmax(l) for batch in preds for l in batch.numpy()]\n",
    "    all_gt  = [l for batch in gt for l in batch.numpy()]\n",
    "\n",
    "    print(classification_report(all_out, all_gt, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
