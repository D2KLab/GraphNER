{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('Device:', device)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = pd.read_csv('conceptnet_en.csv')\n",
    "cn_isa = pd.read_csv('data/conceptnet_isa.csv')\n",
    "cn_keys = set(cn.subject.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2labels = pickle.load(open('edges/word2labels.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2labels['jacob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conll_dataset['train']), len(conll_dataset['validation']), len(conll_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_embeddings = {'hope_gsvd': None,\n",
    "                    'lap_eigmap_svd': None,\n",
    "                    'lle_svd': None,\n",
    "                    'node2vec_rw': None}\n",
    "\n",
    "for embedding_name in nodes_embeddings:\n",
    "    nodes_embeddings[embedding_name] = pickle.load(open('edges/'+embedding_name+'_all_embeddings.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, dataset, dataset_split, nodes_embeddings, window_size = 2):\n",
    "        'Initialization'\n",
    "        RAW, X, Y = [], [], []\n",
    "        for doc in tqdm(dataset[dataset_split], desc=f'Loading split {dataset_split}'):\n",
    "            text = [w.lower() for w in doc['words']]\n",
    "            for i, (token, pos, label) in enumerate(zip(doc['words'], doc['pos'], doc['ner'])):\n",
    "                if token == pos:\n",
    "                    continue # this is punctuation\n",
    "\n",
    "                \n",
    "\n",
    "                if token.endswith('='):\n",
    "                    token = token[:-1]\n",
    "\n",
    "                while token and token[0] in \"!$%&'*+,-.:;<=>?@`\":\n",
    "                    token = token[1:]\n",
    "\n",
    "                token = re.sub(r'\\d+', '<NUM>', token)\n",
    "                token = token.replace('`', \"'\")\n",
    "                \n",
    "                if token.lower() not in nodes_embeddings: # new words appearing only in the eval and test\n",
    "                    token = '<span>'\n",
    "                \n",
    "                if not token:\n",
    "                    continue\n",
    "\n",
    "                extra = ['<'+pos.lower()+'>' if pos.lower() in nodes_embeddings else '<span>']\n",
    "                if token.lower() in word2labels:\n",
    "                    extra.extend(['<'+l.lower()+'>' for l in word2labels[token.lower()]])\n",
    "                if token.lower() not in cn_keys:\n",
    "                    extra.append('<not_in_dict>')\n",
    "                if token == token.upper():\n",
    "                    extra.append('<all_caps>')\n",
    "                if token.count('.') > 0 and (token.count('.') + 1) == len(token.split('.')): # C.J or C.J.\n",
    "                    extra.append('<accronym>')\n",
    "                if token[0] == token[0].upper() and token[1:] == token[1:].lower(): \n",
    "                    extra.append('<capitalized>')\n",
    "                \n",
    "                \n",
    "                left_context  = text[max(i-window_size, 0):i] + ([] if i >= window_size else ['<span>'])\n",
    "                left_context  = [w if w in nodes_embeddings else '<span>' for w in left_context]\n",
    "                \n",
    "                right_context = text[i+1:i+1+window_size] + ([] if i + window_size < len(text) else ['<span>'])\n",
    "                right_context = [w if w in nodes_embeddings else '<span>' for w in right_context]\n",
    "                \n",
    "                graph_rep = np.concatenate([nodes_embeddings[token.lower()],\n",
    "                                            np.mean([nodes_embeddings[w] for w in left_context], axis=0),\n",
    "                                            np.mean([nodes_embeddings[w] for w in right_context], axis=0),\n",
    "                                            np.mean([nodes_embeddings[w] for w in extra], axis=0),\n",
    "                                           ])\n",
    "                X.append(graph_rep)\n",
    "                Y.append(label.split('-')[-1])\n",
    "                RAW.append((token, left_context, right_context, extra))\n",
    "                \n",
    "        \n",
    "        self.X = np.array(X)\n",
    "        self.labels = sorted(set(Y))\n",
    "        self.y2index = {l: i for i, l in enumerate(self.labels)}\n",
    "        self.Y = np.array([self.y2index[y] for y in Y])\n",
    "        self.RAW = RAW\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "    \n",
    "    def get_raw_item(self, index):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.RAW[index]\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_Y(self):\n",
    "        return self.Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x = self.X[index] #.to('cuda') # [:voc_size]\n",
    "        y = self.Y[index]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 64\n",
    "num_workers = 4\n",
    "embeddings_to_use = 'lle_svd'\n",
    "\n",
    "train_set = Dataset(conll_dataset, 'train', nodes_embeddings[embeddings_to_use])\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True) # , sampler=sampler) #\n",
    "\n",
    "dev_set = Dataset(conll_dataset, 'validation', nodes_embeddings[embeddings_to_use])\n",
    "dev_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_set = Dataset(conll_dataset, 'test', nodes_embeddings[embeddings_to_use])\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_set.get_labels()\n",
    "label_counter   = Counter([labels[y] for y in train_set.get_Y()])\n",
    "labels_freqs    = [label_counter[label] / sum(label_counter.values()) for label in labels]\n",
    "labels_weights  = [min(label_counter.values()) / label_counter[label] for label in labels]\n",
    "labels_weights2 = [np.sqrt(min(label_counter.values())) / np.sqrt(label_counter[label]) for label in labels]\n",
    "\n",
    "# sampling_probs = [labels_weights2[labels_to_id[l]] for l in Y_train]\n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(sampling_probs, len(Y_train), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "print(len(train_loader))\n",
    "for batch_X, batch_Y in train_loader:\n",
    "    print(batch_X.shape)\n",
    "    print(batch_Y.shape)\n",
    "    print(sum(batch_X[0]))\n",
    "    print('Class distribution in this batch:', Counter(batch_Y.numpy()))\n",
    "    break\n",
    "print(f'time: {time.time() - t:.3}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.get_raw_item(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=None,filename_suffix='secondattemptwithn2vembeddings')\n",
    "\n",
    "def backprop(batch_X, batch_Y, model, optimizer, loss_fn):\n",
    "    Y_hat = model(batch_X)\n",
    "    loss = loss_fn(Y_hat, batch_Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "input_dim = 1200\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=input_dim, hidden_dim=512, output_dim=5, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fch = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # extra layers layers\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        # self.batchnorm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        # self.batchnorm2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        #x = self.fch2(x)\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.relu(x)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {'loss/train': {}, 'dev': {}}\n",
    "\n",
    "ffnet = FeedForwardNetwork().to('cuda')\n",
    "\n",
    "log_interval = int(len(train_loader) / 4)\n",
    "weights = torch.Tensor(labels_weights2).to('cuda')\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_params = {'lr': 1e-4, \n",
    "                    'momentum': 0.9, \n",
    "                    'weight_decay': 5e-4,\n",
    "                   }\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.SGD(ffnet.parameters(), **optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_epochs = 20\n",
    "\n",
    "for epoch in range(len(logs['loss/train']), len(logs['loss/train']) + max_epochs):\n",
    "    \n",
    "    # Training\n",
    "    ffnet.train()\n",
    "    print('Epoch', epoch)\n",
    "    logs['loss/train'][epoch] = []\n",
    "    writer.add_scalar(\"Learning_rate\", optimizer_params['lr'], epoch)\n",
    "\n",
    "    for batch, (batch_X, batch_Y) in enumerate(tqdm(train_loader)):\n",
    "        # tranfer to GPU\n",
    "        batch_X, batch_Y = batch_X.float().to(device), batch_Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        l = backprop(batch_X, batch_Y, ffnet, optimizer, loss_fn)\n",
    "        logs['loss/train'][epoch].append(l)\n",
    "        \n",
    "        if batch % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(batch_X), len(train_loader.dataset),\n",
    "                100. * batch / len(train_loader), l))\n",
    "    \n",
    "    logs['loss/train'][epoch] = np.mean(logs['loss/train'][epoch])\n",
    "    writer.add_scalar(\"Loss/train\", logs['loss/train'][epoch], epoch)\n",
    "    print(f'Average loss on epoch {epoch}: {logs[\"loss/train\"][epoch]}')\n",
    "    \n",
    "    # Validation\n",
    "    ffnet.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        gt = []\n",
    "        for batch, (batch_X, batch_Y) in enumerate(tqdm(dev_loader)):\n",
    "            # Transfer to GPU\n",
    "            batch_X = batch_X.float().to(device)\n",
    "            output = nn.Softmax(dim=1)(ffnet(batch_X))\n",
    "            preds.append(output.cpu())\n",
    "            gt.append(batch_Y)\n",
    "    \n",
    "        all_out = [np.argmax(l) for batch in preds for l in batch.numpy()]\n",
    "        all_gt  = [l for batch in gt for l in batch.numpy()]\n",
    "        \n",
    "        print(classification_report(all_out, all_gt, digits=4))\n",
    "        micro_F1 = metrics.f1_score(all_gt, all_out, average='micro')\n",
    "        macro_F1 = metrics.f1_score(all_gt, all_out, average='macro')\n",
    "        weighted_F1 = metrics.f1_score(all_gt, all_out, average='weighted')\n",
    "        writer.add_scalar(\"micro_F1/dev\", micro_F1, epoch)\n",
    "        writer.add_scalar(\"macro_F1/dev\", macro_F1, epoch)\n",
    "        writer.add_scalar(\"weighted_F1/dev\", weighted_F1, epoch)\n",
    "        logs['dev'][epoch] = (micro_F1, weighted_F1, macro_F1, (all_gt, all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnet.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    gt = []\n",
    "    for batch, (batch_X, batch_Y) in enumerate(tqdm(test_loader)):\n",
    "        # Transfer to GPU\n",
    "        batch_X = batch_X.float().to(device)\n",
    "        output = nn.Softmax(dim=1)(ffnet(batch_X))\n",
    "        preds.append(output.cpu())\n",
    "        gt.append(batch_Y)\n",
    "\n",
    "    all_out = [np.argmax(l) for batch in preds for l in batch.numpy()]\n",
    "    all_gt  = [l for batch in gt for l in batch.numpy()]\n",
    "\n",
    "    print(classification_report(all_out, all_gt, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
