{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoNLL 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.94 s, sys: 522 ms, total: 5.46 s\n",
      "Wall time: 5.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cn = pd.read_csv('conceptnet_en.csv')[['subject', 'relation', 'object']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {'train': 'data/eng.train', 'dev': 'data/eng.testa', 'test': 'data/eng.testb'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "dev\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "paths = {'train': 'data/eng.train', 'dev': 'data/eng.testa', 'test': 'data/eng.testb'}\n",
    "data, vocabulary = {}, {}\n",
    "\n",
    "for split, path in paths.items():\n",
    "    print(split)\n",
    "    data[split] = []\n",
    "    vocabulary[split] = set()\n",
    "    \n",
    "    with open(path) as file:\n",
    "        tokens, labels = [], []\n",
    "        \n",
    "        for line in file:\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                continue\n",
    "            elif line.strip():\n",
    "                token, _, _, label = line.split(' ')\n",
    "                vocabulary[split].add(token)\n",
    "                tokens.append(token)\n",
    "                labels.append(label[:-1])\n",
    "                \n",
    "            elif tokens and labels:\n",
    "                data[split].append((tokens, labels))\n",
    "                tokens, labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabulary = set(vocabulary['train']).union(set(vocabulary['dev']).union(set(vocabulary['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['834405', '13.', '1996-12-06', '.8787', 'date:August', '1:55.281', '203', 'razzle-dazzle', '6.0-percent', '6000', 'close-knit', '1:54.984', '7.83', '1,806', '+0.9;+23.6', 'Owen-Jones', '8.3', 're-election', '9,306,000', '3404.71', '480.72', '0:09', 'U.S', '100=1992', '597', '15-6', '30.', '1:51.04', '.444', 'anti-guerrilla', 'asset-St', '1966-76', '77.06', '4:05.00', '9.', '257', '48-43', '45.34', 'oil-based', '27,600', '12-2', '+65-8703086', 'volcano-hit', '1,037', '3:42.852', '38:34.999', '30,300-30,400', '0.65', '.578', '1997-98', 'Above-normal', '13-year', '68.44', '2,389', '3.89', '11.', 'first-inning', '21.25', '36-year-old', 'Mont-sur-Marchienne', '12-match', 'Hindu-Moslem', '222,457', '67.5', '68.80', '28,000', '5,120', '59440', '87.45', '55-11-2324411', 'no-nonsense', 'KwaZulu-Natal', 'well-oiled', '5522', 'PRO-LEAGUE', '42-36', 'higher-than-average', '6mth', '9.77', '31.44', '11-6', 'F.W.', '48.61', '10-0-52-0', '26:28.2', '33-26', '3.92', '1990', 'left-footed', '13.45', '14-28', '8.07', '15.125', 'b-9', 'league-leading', '0', 'eight-year-old', '49.2', '38,915.87', '8,200']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in all_vocabulary if not w.isalpha()][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-acf0272f7602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ORG'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'entities' is not defined"
     ]
    }
   ],
   "source": [
    "print([w for w in entities['ORG'] if not w.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-780940b26e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'entities' is not defined"
     ]
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split \t |N| \t Unique\n",
      "---------------------------\n",
      "train \t 203621\t 23623\n",
      "dev \t 51362\t 9966\n",
      "test \t 46435\t 9488\n",
      "All \t 301418\t 30289\n"
     ]
    }
   ],
   "source": [
    "print('Split', '\\t', '|N|', '\\t', 'Unique')\n",
    "print('---------------------------')\n",
    "N = 0\n",
    "word_freq = {}\n",
    "for s in data:\n",
    "    word_freq[s] =  Counter([])\n",
    "    for d in data[s]: word_freq[s] += Counter(d[0])\n",
    "    print(s, '\\t', str(sum(word_freq[s].values())) + '\\t', len(word_freq[s].keys()))\n",
    "    N += sum(word_freq[s].values())\n",
    "all_words_freq = word_freq['train'] + word_freq['dev'] + word_freq['test']\n",
    "print('All', '\\t', str(N) + '\\t', len(all_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split \t |V| \t |v|\n",
      "---------------------------\n",
      "train \t 23623 \t 21009\n",
      "dev \t 9966 \t 9002\n",
      "test \t 9488 \t 8548\n",
      "All \t 30289\t 26869\n"
     ]
    }
   ],
   "source": [
    "print('Split', '\\t', '|V|', '\\t', '|v|')\n",
    "print('---------------------------')\n",
    "for s in vocabulary:\n",
    "    print(s, '\\t', len(vocabulary[s]), '\\t', len(set([t.lower() for t in vocabulary[s]])))\n",
    "print('All', '\\t', str(len(all_vocabulary)) + '\\t', len(set([t.lower() for t in all_vocabulary])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens unique to the devset: 3260\n",
      "Number of tokens unique to the testset: 3693\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens unique to the devset:',len(set([t for t in vocabulary['dev'] if t not in vocabulary['train']])))\n",
    "print('Number of tokens unique to the testset:', len(set([t for t in vocabulary['test'] if t not in vocabulary['train']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {'PER': set(), 'ORG': set(), 'LOC': set(), 'MISC': set()}\n",
    "# entities = set()\n",
    "\n",
    "for split in data:\n",
    "    for tokens, labels in data[split]:\n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label != 'O':\n",
    "                entities[label.split('-')[-1]].add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split \t |E| \t |e|\n",
      "---------------------------\n",
      "PER \t 5555 \t 5404\n",
      "ORG \t 3235 \t 3005\n",
      "LOC \t 1868 \t 1503\n",
      "MISC \t 1343 \t 1211\n"
     ]
    }
   ],
   "source": [
    "print('Split', '\\t', '|E|', '\\t', '|e|')\n",
    "print('---------------------------')\n",
    "for l in entities:\n",
    "    print(l, '\\t', len(entities[l]), '\\t', len(set([e.lower() for e in entities[l]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities_lower = set()\n",
    "for l in entities:\n",
    "    for e in entities[l]:\n",
    "        all_entities_lower.add(e.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10108"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_entities_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1165189"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cn_nodes = set(cn.subject.unique())\n",
    "len(all_cn_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5558"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_nodes_in_cn = [e for e in all_entities_lower if e in all_cn_nodes]\n",
    "len(entity_nodes_in_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15747"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_in_cn = [e for e in [w.lower() for w in vocabulary['train']] if e in all_cn_nodes]\n",
    "len(train_words_in_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_nodes = [e for e in all_entities_lower if e not in all_cn_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clignet',\n",
       " 'keun',\n",
       " 'nabil',\n",
       " 'bagwell',\n",
       " 'stieglmair',\n",
       " 'caltex',\n",
       " 'quequen',\n",
       " 'roest',\n",
       " 'tramacchi',\n",
       " 'gulyayeva',\n",
       " 'kareda',\n",
       " 'bergamo-based',\n",
       " 'batigol',\n",
       " 'eliud',\n",
       " 'jose-maria',\n",
       " 'frontieres',\n",
       " 'baeron',\n",
       " 'hasina',\n",
       " 'zdf',\n",
       " 'danciulescu']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outside_nodes[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-cad614cf99fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcn_isa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'isa'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcn_isa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "cn_isa = cn[(cn.subject.isin(final_vocab)) & (cn.relation == 'isa') & (cn.object.isin(target_classes))].reset_index()\n",
    "cn_isa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cn_isa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-38704b15960d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_word_with_isa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcn_isa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcn_isa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words_in_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_word_with_isa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cn_isa' is not defined"
     ]
    }
   ],
   "source": [
    "train_word_with_isa = cn_isa[cn_isa.subject.isin(train_words_in_cn)].subject.unique()\n",
    "len(train_word_with_isa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cn_isa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-19d6853f1148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn_isa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcn_isa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words_in_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cn_isa' is not defined"
     ]
    }
   ],
   "source": [
    "freq = Counter(cn_isa[cn_isa.subject.isin(train_words_in_cn)].object.values)\n",
    "pd.Series([v for w, v in freq.most_common(100)], index=[w for w, v in freq.most_common(100)]).plot(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w for w, v in freq.most_common(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [1, 2, 3]:\n",
    "    rare_words = [w for w in all_words_freq if all_words_freq[w] <= f]\n",
    "    print('freq <=', f, ':', len(rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cn_isa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2785b4840273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentity_nodes_with_isa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcn_isa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcn_isa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_nodes_in_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_nodes_with_isa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cn_isa' is not defined"
     ]
    }
   ],
   "source": [
    "entity_nodes_with_isa = cn_isa[cn_isa.subject.isin(entity_nodes_in_cn)].subject.unique()\n",
    "len(entity_nodes_with_isa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_freq.most_common(10)23623 \t 21009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonalpha = [w.lower() for w in vocabulary['train'] if len(w) == 1 and not w.isalpha()]\n",
    "final_vocab = sorted(set([w.lower() for w in vocabulary['train'] if w not in nonalpha]))\n",
    "word_to_idx = {w:i for i, w in enumerate(final_vocab + ['[UNK]'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20978"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of properties: 40\n"
     ]
    }
   ],
   "source": [
    "target_classes = sorted( ['person', 'city', 'country', 'book', 'given_name', 'company', 'musical_artist', \n",
    "                          'state', 'film', 'human', 'person_with_nationality', 'album', 'software', \n",
    "                          'administrative_region', 'tangible_thing', 'band', 'activity',  'people', \n",
    "                          'capital', 'organization', 'weapon', 'magazine', 'river', 'part', 'device', 'structure', \n",
    "                          'place', 'town', 'event', 'artifact', 'african', 'area', 'disease', 'soccer_player',\n",
    "                          'station', 'ethnic_group', 'agent_non_geographical', 'region', 'location', 'asian'] )\n",
    "class_to_idx = {c:i for i, c in enumerate(target_classes)}\n",
    "print('Number of properties:', len(target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1087076</td>\n",
       "      <td>11</td>\n",
       "      <td>isa</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1087113</td>\n",
       "      <td>15</td>\n",
       "      <td>isa</td>\n",
       "      <td>band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1087128</td>\n",
       "      <td>16</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1087155</td>\n",
       "      <td>1984</td>\n",
       "      <td>isa</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1087156</td>\n",
       "      <td>1984</td>\n",
       "      <td>isa</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index subject relation  object\n",
       "0  1087076      11      isa    film\n",
       "1  1087113      15      isa    band\n",
       "2  1087128      16      isa  weapon\n",
       "3  1087155    1984      isa    film\n",
       "4  1087156    1984      isa   album"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_isa = cn[(cn.subject.isin(final_vocab)) & (cn.relation == 'isa') & (cn.object.isin(target_classes))].reset_index()\n",
    "cn_isa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20M_classifier_all_available_features.ipynb   hparams.yaml\r\n",
      "conceptnet_en.csv\t\t\t      lightning_logs\r\n",
      "data\t\t\t\t\t      onehot_pytorch_lightning.ipynb\r\n",
      "edge_list_generation.ipynb\t\t      prepare_dataset.ipynb\r\n",
      "graph_embeddings_generation.ipynb\t      README.md\r\n",
      "GraphNER_binary_representation_pytorch.ipynb  runs\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1087076</td>\n",
       "      <td>11</td>\n",
       "      <td>isa</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1087113</td>\n",
       "      <td>15</td>\n",
       "      <td>isa</td>\n",
       "      <td>band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1087128</td>\n",
       "      <td>16</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1087155</td>\n",
       "      <td>1984</td>\n",
       "      <td>isa</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1087156</td>\n",
       "      <td>1984</td>\n",
       "      <td>isa</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1087187</td>\n",
       "      <td>1st</td>\n",
       "      <td>isa</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1087216</td>\n",
       "      <td>201</td>\n",
       "      <td>isa</td>\n",
       "      <td>device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1087252</td>\n",
       "      <td>25</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1087257</td>\n",
       "      <td>26</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1087262</td>\n",
       "      <td>28</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1087263</td>\n",
       "      <td>28</td>\n",
       "      <td>isa</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1087335</td>\n",
       "      <td>30</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1087341</td>\n",
       "      <td>31</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1087346</td>\n",
       "      <td>32</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1087352</td>\n",
       "      <td>33</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1087356</td>\n",
       "      <td>34</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1087361</td>\n",
       "      <td>37</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1087370</td>\n",
       "      <td>38</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1087374</td>\n",
       "      <td>39</td>\n",
       "      <td>isa</td>\n",
       "      <td>weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1087408</td>\n",
       "      <td>3m</td>\n",
       "      <td>isa</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index subject relation   object\n",
       "0   1087076      11      isa     film\n",
       "1   1087113      15      isa     band\n",
       "2   1087128      16      isa   weapon\n",
       "3   1087155    1984      isa     film\n",
       "4   1087156    1984      isa    album\n",
       "5   1087187     1st      isa    album\n",
       "6   1087216     201      isa   device\n",
       "7   1087252      25      isa   weapon\n",
       "8   1087257      26      isa   weapon\n",
       "9   1087262      28      isa   weapon\n",
       "10  1087263      28      isa     book\n",
       "11  1087335      30      isa   weapon\n",
       "12  1087341      31      isa   weapon\n",
       "13  1087346      32      isa   weapon\n",
       "14  1087352      33      isa   weapon\n",
       "15  1087356      34      isa   weapon\n",
       "16  1087361      37      isa   weapon\n",
       "17  1087370      38      isa   weapon\n",
       "18  1087374      39      isa   weapon\n",
       "19  1087408      3m      isa  company"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cn_isa.to_csv('data/conceptnet_isa.csv')\n",
    "cn_isa.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: impossible de créer le répertoire « data/ws3_40r »: Le fichier existe\n",
      "mkdir: impossible de créer le répertoire « data/ws3_40r/train »: Le fichier existe\n",
      "mkdir: impossible de créer le répertoire « data/ws3_40r/dev »: Le fichier existe\n",
      "mkdir: impossible de créer le répertoire « data/ws3_40r/test »: Le fichier existe\n",
      "eng.testa\t   eng.testb\t      eng.train\t\t ws3_40r\n",
      "eng.testa.openNLP  eng.testb.openNLP  eng.train.openNLP\n"
     ]
    }
   ],
   "source": [
    "! mkdir data/ws3_40r\n",
    "! mkdir data/ws3_40r/train\n",
    "! mkdir data/ws3_40r/dev\n",
    "! mkdir data/ws3_40r/test\n",
    "\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_token(word_idx, context, \n",
    "                 df_props=cn_isa, \n",
    "                 word_to_idx=word_to_idx, \n",
    "                 class_to_idx=class_to_idx, \n",
    "                 window_size=window_size):\n",
    "    \n",
    "    voc_size = len(word_to_idx)\n",
    "    word = context[word_idx]\n",
    "    \n",
    "    n_extra = 4 #sparse_data['train']\n",
    "    is_notincn = 1 if word.lower() in all_cn_nodes else 0 \n",
    "    is_allcaps = 1 if word == word.upper() else 0\n",
    "    is_initials = word.count('.') > 0 and (word.count('.') + 1) == len(word.split('.')) # C.J or C.J.\n",
    "    is_capitalized = 1 if word[0] == word[0].upper() else 0\n",
    "\n",
    "    vec_size = voc_size * 3 + len(class_to_idx) + n_extra\n",
    "    \n",
    "    node  = [word_to_idx[word.lower() if word.lower() in word_to_idx else '[UNK]']]\n",
    "    left  = [voc_size * 1 + (word_to_idx[w.lower()] if w.lower() in word_to_idx else word_to_idx['[UNK]']) \n",
    "             for w in context[max(0, word_idx-window_size):word_idx]]\n",
    "    right = [voc_size * 2  + (word_to_idx[w.lower()] if w.lower() in word_to_idx else word_to_idx['[UNK]']) \n",
    "             for w in context[word_idx+1:min(len(context)-1, word_idx+window_size+1)]]\n",
    "    props = [voc_size * 3 + class_to_idx[c] for c in df_props[df_props.subject == word.lower()].object.values]\n",
    "    extra = list(range(vec_size - n_extra, vec_size))\n",
    "    \n",
    "    i = torch.LongTensor([node + left + right + props + extra])\n",
    "    v = torch.FloatTensor([1] * len(node + left + right + props) + [is_notincn, is_allcaps, is_initials, is_capitalized])\n",
    "    vec = torch.sparse.FloatTensor(i, v, torch.Size([vec_size]))\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20979, 41958, 62937)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = len(word_to_idx)\n",
    "voc_size, voc_size * 2, voc_size * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62981])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_token(2, ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']).to_dense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating data for TRAIN\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc7bc0ab3794b328f98d87df0d852e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=14041.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Generating data for DEV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9e553e103348f481b949a36418c441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='dev', max=3250.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Generating data for TEST\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668d645286134c4d85c01af5d737d079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='test', max=3453.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_data = {}\n",
    "for split in data:\n",
    "    print('\\n\\nGenerating data for', split.upper())\n",
    "    sparse_data[split] = []\n",
    "    for tokens, labels in tqdm(data[split], desc=split):\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in nonalpha: \n",
    "                continue\n",
    "            \n",
    "            vec = encode_token(i, tokens, window_size=2)\n",
    "            idx = vec._indices()[0].numpy()\n",
    "            sample = (vec, labels[i].split('-')[-1])\n",
    "            # print(' '.join(final_vocab[ix] for ix in idx if ix < voc_size), sample)\n",
    "            sparse_data[split].append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(indices=tensor([[ 9846, 29725, 37330, 47920, 61284, 62957, 62964, 62965,\n",
       "                         62977, 62978, 62979, 62980]]),\n",
       "        values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.]),\n",
       "        size=(62981,), nnz=12, layout=torch.sparse_coo),\n",
       " 'MISC')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_data['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003_sparse_dev.pickle    eng.testa\t  eng.testb.openNLP  ws2_40r\r\n",
      "conll2003_sparse_test.pickle   eng.testa.openNLP  eng.train\t     ws3_40r\r\n",
      "conll2003_sparse_train.pickle  eng.testb\t  eng.train.openNLP\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sparse_data['train'], open('data/ws2_40r/conll2003_sparse_train.pickle', 'wb'))\n",
    "pickle.dump(sparse_data['dev'], open('data/ws2_40r/conll2003_sparse_dev.pickle', 'wb'))\n",
    "pickle.dump(sparse_data['test'], open('data/ws2_40r/conll2003_sparse_test.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_vocab + ['[UNK]'], open('data/ws2_40r/vocab_list.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
