{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = pd.read_csv('conceptnet_en.csv')\n",
    "cn_isa = pd.read_csv('data/conceptnet_isa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_isa.object.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_keys = set(cn.subject.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cn_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn.subject.values[1800:1823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2labels = {}\n",
    "for w in cn_isa.subject.unique():\n",
    "    subcn = cn_isa[cn_isa.subject == w]\n",
    "    word2labels[w] = subcn.object.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word2labels, open('edges/word2labels.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['train']['words'][:3], dataset['train']['pos'][:3], dataset['train']['ner'][:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(['!', '$', '%', '&', \"'\", '*', '+', ',', '-', '.', ':', ';', '<', '=', '>', '?', '@', '`'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "punctuation = ['!', '$', '%', '&', \"'\", '*', '+', ',', '-', '.', ':', ';', '<', '=', '>', '?', '@', '`']\n",
    "\n",
    "vocabulary = {}\n",
    "data = {}\n",
    "\n",
    "special_cases = []\n",
    "special_num_cases = []\n",
    "special_O_cases = []\n",
    "\n",
    "for split in['train', 'validation', 'test']:\n",
    "    # print(split)\n",
    "    data[split] = []\n",
    "    vocabulary[split] = set()\n",
    "    \n",
    "    for doc in tqdm(dataset[split], desc=split.upper()):\n",
    "        tokens, labels, extras = [], [], []\n",
    "        \n",
    "        for token, pos, label in zip(doc['words'], doc['pos'], doc['ner']):\n",
    "            if token == pos:\n",
    "                continue # this is punctuation\n",
    "            \n",
    "            elif pos == ',':\n",
    "                pos = 'NNP'\n",
    "            \n",
    "            if token.endswith('='):\n",
    "                token = token[:-1]\n",
    "            \n",
    "            while token and token[0] in punctuation:\n",
    "                token = token[1:]\n",
    "            \n",
    "            token = re.sub(r'\\d+', '<NUM>', token)\n",
    "            token = token.replace('`', \"'\")\n",
    "            \n",
    "            if not token:\n",
    "                continue\n",
    "            \n",
    "            if all([c in ',.-' for c in token.split('<NUM>')]):\n",
    "                special_num_cases.append((token, label))\n",
    "                token = '<NUM>'\n",
    "            elif not token.isalpha() and label != 'O':\n",
    "                special_cases.append((token, label))\n",
    "            elif not token.isalpha() and label == 'O':\n",
    "                special_O_cases.append((token, label))\n",
    "            \n",
    "            extra = ['<'+pos.lower()+'>']\n",
    "            if token.lower() in word2labels:\n",
    "                extra.extend(['<'+l.lower()+'>' for l in word2labels[token.lower()]])\n",
    "                \n",
    "            if token.lower() not in cn_keys:\n",
    "                extra.append('<not_in_dict>')\n",
    "            if token == token.upper():\n",
    "                extra.append('<all_caps>')\n",
    "            if token.count('.') > 0 and (token.count('.') + 1) == len(token.split('.')): # C.J or C.J.\n",
    "                extra.append('<accronym>')\n",
    "            if token[0] == token[0].upper() and token[:1] == token[:1].lower(): \n",
    "                extra.append('<capitalized>')\n",
    "                \n",
    "                \n",
    "            vocabulary[split].add(token.lower())\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "            extras.append(extra)\n",
    "            \n",
    "\n",
    "        data[split].append((tokens, labels, extras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_vocab = list(set([e for example in data['train'] for l in example[2] for e in l]))\n",
    "print(extra_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(set([x[0] for x in special_O_cases]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_voc = set([w for split in vocabulary for w in vocabulary[split]])\n",
    "print(len(all_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(vocabulary[split]) for split in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [x for l in data['train'] for x in l[0]]\n",
    "train_counter = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_counter = Counter([x for l in data['test'] for x in l[1]])\n",
    "train_labels_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intials = []\n",
    "accronyms = []\n",
    "whatelse = []\n",
    "hyphenated = []\n",
    "\n",
    "for term, label in special_cases:\n",
    "    if term == term.upper() and term.count('.') > 0 and term.count('.') == len(term.split('.')) - 1 and len(term) <= 2:\n",
    "        intials.append((term, label))\n",
    "    elif term == term.upper() and term.count('.') > 0 and term.count('.') == len(term.split('.')) - 1 and len(term) > 2:\n",
    "        accronyms.append((term, label))\n",
    "    elif '-' in term and len(term.split('-')) > 1 and  (term.split('-')[0] == term.split('-')[0].lower() or (term.split('-')[1] == term.split('-')[1].lower())):\n",
    "        hyphenated.append((term, label))\n",
    "    else:\n",
    "        whatelse.append((term, label))\n",
    "# print('\\n'.join(str(c) for c in set(whatelse)))\n",
    "print(len(whatelse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = sorted(vocabulary['train']) + ['<span>'] + sorted(extra_vocab)\n",
    "len(final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_vocab[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {w:i for i,w in enumerate(final_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id['ismail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_vocab, open('edges/vocabulary.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_edges = {w: [] for w in final_vocab}\n",
    "after_edges  = {w: [] for w in final_vocab}\n",
    "isa_edges    = {w: [] for w in final_vocab}\n",
    "vocab_dict   = {w: [] for w in final_vocab}\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "for split in data:\n",
    "    for example in tqdm(data[split], desc=split.upper()):\n",
    "        text = [w.lower() for w in example[0]]\n",
    "        for i, word in enumerate(text):\n",
    "            term = word.lower()\n",
    "            if term not in vocab_dict: # new words appearing only in the eval and test\n",
    "                term = '<span>'\n",
    "            left_context  = text[max(i-window_size, 0):i] + ([] if i >= window_size else ['<span>'])\n",
    "            right_context = text[i+1:i+1+window_size] + ([] if i + window_size < len(text) else ['<span>'])\n",
    "            left_context  = [w if w in vocab_dict else '<span>' for w in left_context]\n",
    "            right_context = [w if w in vocab_dict else '<span>' for w in right_context]\n",
    "            isa_context   = example[2][i]\n",
    "            \n",
    "            before_edges[term].extend(right_context)\n",
    "            after_edges[term].extend(left_context)\n",
    "            isa_edges[term].extend(isa_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[l[0] for l in sorted(isa_edges.items(), key=lambda k: len(set(k[1])), reverse=True)[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list_before = []\n",
    "edge_list_after  = []\n",
    "edge_list_isa    = []\n",
    "\n",
    "for word in vocab_dict:\n",
    "    edge_list_before.extend((word2id[word], word2id[w]) for w in before_edges[word])\n",
    "    edge_list_after.extend((word2id[word], word2id[w]) for w in after_edges[word])\n",
    "    edge_list_isa.extend((word2id[word], word2id[w]) for w in isa_edges[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge_list_before), len(edge_list_after), len(edge_list_isa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(edge_list_before)), len(set(edge_list_after)), len(set(edge_list_isa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all_edges = len(set(edge_list_before)) + len(set(edge_list_after)) + len(set(edge_list_isa))\n",
    "edges_list_unique = sorted(set(edge_list_before).union(set(edge_list_after)))\n",
    "len_unique_context_edges = len(edges_list_unique)\n",
    "len_all_edges, len_unique_context_edges, len_all_edges - len_unique_context_edges - len(set(edge_list_isa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list_all = sorted(set(edge_list_before).union(set(edge_list_after).union(set(edge_list_isa))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(edge_list_before).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab[7470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_lists = {'before_edges': edge_list_before,\n",
    "              'after_edges': edge_list_after,\n",
    "              'isa_edges': edge_list_isa,\n",
    "              'context_edges': edges_list_unique,\n",
    "              'all_edges': edges_list_all,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in edge_lists:\n",
    "    pickle.dump(edge_lists[filename], open('edges/' + filename + '.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in edge_lists:\n",
    "    with open('edges/' + filename + '.edgelist', 'w') as f:\n",
    "        for s, t in edge_lists[filename]:\n",
    "            f.write(f'{s} {t}\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
